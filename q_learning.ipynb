{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train function\n",
    "\n",
    "def q_learning(Q, env, n_train_episodes, lr, max_steps, gamma, min_epsilon, max_epsilon, decay):\n",
    "    epsilon = 1\n",
    "\n",
    "    for episode in range(n_train_episodes):\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay * episode)\n",
    "        state = env.reset() # get env state\n",
    "\n",
    "        # print(\"state type: \", type(state))\n",
    "        # if state is of type tuple, convret it to a index\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "\n",
    "        for step in range(max_steps):\n",
    "\n",
    "            # choose an action based on a random number - exploration-exploitation trade-off\n",
    "            if random.uniform(0, 1) > epsilon:\n",
    "                action = np.argmax(Q[state,:])  # exploit\n",
    "            else:\n",
    "                action = env.action_space.sample()  # explore\n",
    "\n",
    "            # perform the action\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = truncated | terminated        \n",
    "\n",
    "            # update the Q-table with Bellman equation\n",
    "            Q[state, action] = Q[state, action] + lr * (reward + gamma * np.max(Q[new_state,:]) - Q[state, action])\n",
    "\n",
    "            # end the episode\n",
    "            if done == True:\n",
    "                break\n",
    "            \n",
    "            # update the state\n",
    "            state = new_state\n",
    "\n",
    "    print(\"Training completed over\", n_train_episodes, \"episodes\")\n",
    "    return Q, env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "\n",
    "def q_evaluation(Q, env, n_eval_episodes, lr, max_steps, gamma, min_epsilon, max_epsilon, decay):\n",
    "    for episode in range(n_eval_episodes):\n",
    "        state = env.reset() # get env state  \n",
    "        \n",
    "        # if state is of type tuple, convret it to a index\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]      \n",
    "\n",
    "        for step in range(max_steps):\n",
    "\n",
    "            action = np.argmax(Q[state,:])\n",
    "\n",
    "            # perform the action\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = truncated | terminated        \n",
    "\n",
    "            # update the Q-table with Bellman equation\n",
    "            Q[state, action] = Q[state, action] + lr * (reward + gamma * np.max(Q[new_state, action]) - Q[state, action])\n",
    "            # update the state\n",
    "            state = new_state\n",
    "\n",
    "            # end the episode\n",
    "            if done == True:\n",
    "                break\n",
    "    \n",
    "    # state = env.reset() # get env state      \n",
    "    # for step in range(max_steps):\n",
    "    #     if isinstance(state, tuple):\n",
    "    #         state = state[0]\n",
    "    #     action = np.argmax(Q[state, :])\n",
    "    #     new_state, reward, terminated, truncated, info = env.step(action)\n",
    "    #     done = truncated | terminated\n",
    "    #     state = new_state\n",
    "\n",
    "    #     if done == True:\n",
    "    #         break\n",
    "            \n",
    "    print(\"Evaluation completed over\", n_eval_episodes, \"episodes\")\n",
    "    return Q, env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gif(Q, env, filename):\n",
    "    frames = []\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "        action = np.argmax(Q[state,:])\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = truncated | terminated\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "        state = new_state\n",
    "\n",
    "    imageio.mimsave(filename, frames, fps = 6)\n",
    "    print(filename, \"GIF saved\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FrozenLake 4x4\n",
    "\n",
    "def frozen_lake_4x4():\n",
    "    env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")\n",
    "    env.reset()\n",
    "\n",
    "    # Setting the parameters\n",
    "    n_train_episodes = 10000\n",
    "    lr = 0.7    # learning rate\n",
    "    n_eval_episodes = 100\n",
    "    max_steps = 100\n",
    "    gamma = 0.95\n",
    "    min_epsilon = 0.05\n",
    "    max_epsilon = 1.0\n",
    "    decay = 0.0005\n",
    "\n",
    "    # Initialize the Q-table\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    Q, env = q_learning(Q, env, n_train_episodes, lr, max_steps, gamma, min_epsilon, max_epsilon, decay)\n",
    "    Q, env = q_evaluation(Q, env, n_eval_episodes, lr, max_steps, gamma, min_epsilon, max_epsilon, decay)\n",
    "\n",
    "    create_gif(Q, env, \"FrozenLake-v1-4x4.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FrozenLake 4x4 Slippery\n",
    "\n",
    "def frozen_lake_4x4_slippery():\n",
    "    env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True, render_mode=\"rgb_array\")\n",
    "    env.reset()\n",
    "\n",
    "    # Setting the parameters\n",
    "    n_train_episodes = 10000\n",
    "    lr = 0.7    # learning rate\n",
    "    n_eval_episodes = 100\n",
    "    max_steps = 100\n",
    "    gamma = 0.95\n",
    "    min_epsilon = 0.05\n",
    "    max_epsilon = 1.0\n",
    "    decay = 0.0005\n",
    "\n",
    "    # Initialize the Q-table\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    Q, env = q_learning(Q, env, n_train_episodes, lr, max_steps, gamma, min_epsilon, max_epsilon, decay)\n",
    "    Q, env = q_evaluation(Q, env, n_eval_episodes, lr, max_steps, gamma, min_epsilon, max_epsilon, decay)\n",
    "\n",
    "    create_gif(Q, env, \"FrozenLake-v1-4x4-slippery.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FrozenLake 8x8\n",
    "\n",
    "def frozen_lake_8x8():\n",
    "    env = gym.make(\"FrozenLake-v1\", map_name=\"8x8\", is_slippery=False, render_mode=\"rgb_array\")\n",
    "    env.reset()\n",
    "\n",
    "    # Setting the parameters\n",
    "    n_train_episodes = 250000\n",
    "    lr = 0.8    # learning rate\n",
    "    n_eval_episodes = 1000\n",
    "    max_steps = 400\n",
    "    gamma = 0.9\n",
    "    min_epsilon = 0.001\n",
    "    max_epsilon = 1.0\n",
    "    decay = 0.00005\n",
    "\n",
    "    # Initialize the Q-table\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    Q, env = q_learning(Q, env, n_train_episodes, lr, max_steps, gamma, min_epsilon, max_epsilon, decay)\n",
    "    Q, env = q_evaluation(Q, env, n_eval_episodes, lr, max_steps, gamma, min_epsilon, max_epsilon, decay)\n",
    "\n",
    "    create_gif(Q, env, \"FrozenLake-v1-8x8.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FrozenLake 8x8 Slippery\n",
    "\n",
    "def frozen_lake_8x8_slippery():\n",
    "    env = gym.make(\"FrozenLake-v1\", map_name=\"8x8\", is_slippery=True, render_mode=\"rgb_array\")\n",
    "    env.reset()\n",
    "\n",
    "    # Setting the parameters\n",
    "    n_train_episodes = 250000\n",
    "    lr = 0.8    # learning rate\n",
    "    n_eval_episodes = 1000\n",
    "    max_steps = 400\n",
    "    gamma = 0.9\n",
    "    min_epsilon = 0.001\n",
    "    max_epsilon = 1.0\n",
    "    decay = 0.00005\n",
    "\n",
    "    # Initialize the Q-table\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    Q, env = q_learning(Q, env, n_train_episodes, lr, max_steps, gamma, min_epsilon, max_epsilon, decay)\n",
    "    Q, env = q_evaluation(Q, env, n_eval_episodes, lr, max_steps, gamma, min_epsilon, max_epsilon, decay)\n",
    "\n",
    "    create_gif(Q, env, \"FrozenLake-v1-8x8-slippery.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taxi\n",
    "\n",
    "def taxi():\n",
    "    env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\n",
    "    env.reset()\n",
    "\n",
    "    # Setting the parameters\n",
    "    n_train_episodes = 10000\n",
    "    lr = 0.7    # learning rate\n",
    "    n_eval_episodes = 100\n",
    "    max_steps = 100\n",
    "    gamma = 0.95\n",
    "    min_epsilon = 0.05\n",
    "    max_epsilon = 1.0\n",
    "    decay = 0.0005\n",
    "\n",
    "    # Initialize the Q-table\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n)) # 500 states, 6 actions\n",
    "    Q, env = q_learning(Q, env, n_train_episodes, lr, max_steps, gamma, min_epsilon, max_epsilon, decay)\n",
    "    Q, env = q_evaluation(Q, env, n_eval_episodes, lr, max_steps, gamma, min_epsilon, max_epsilon, decay)\n",
    "\n",
    "    create_gif(Q, env, \"Taxi-v3.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CliffWalking\n",
    "\n",
    "def cliff_walking():\n",
    "    env = gym.make(\"CliffWalking-v0\", render_mode=\"rgb_array\")\n",
    "    env.reset()\n",
    "\n",
    "    # Setting the parameters\n",
    "    n_train_episodes = 10000\n",
    "    lr = 0.7    # learning rate\n",
    "    n_eval_episodes = 100\n",
    "    max_steps = 100\n",
    "    gamma = 0.95\n",
    "    min_epsilon = 0.05\n",
    "    max_epsilon = 1.0\n",
    "    decay = 0.0005\n",
    "\n",
    "    # Initialize the Q-table\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    Q, env = q_learning(Q, env, n_train_episodes, lr, max_steps, gamma, min_epsilon, max_epsilon, decay)\n",
    "    Q, env = q_evaluation(Q, env, n_eval_episodes, lr, max_steps, gamma, min_epsilon, max_epsilon, decay)\n",
    "\n",
    "    create_gif(Q, env, \"CliffWalking-v0.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_lake_4x4()\n",
    "frozen_lake_4x4_slippery()\n",
    "cliff_walking()\n",
    "taxi()\n",
    "frozen_lake_8x8()\n",
    "frozen_lake_8x8_slippery()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
